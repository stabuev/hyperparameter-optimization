{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение: Автоматическая оптимизация гиперпараметров\n",
    "\n",
    "В этом ноутбуке е мы рассмотрим автоматическую настройку гиперпараметров с помощью байесовской оптимизации. В частности, мы оптимизируем гиперпараметры Gradient Boosting Machine, используя библиотеку Hyperopt (с алгоритмом оценки Tree Parzen Estimator). Мы сравним результаты случайного поиска (реализованного вручную) для настройки гиперпараметров с методом оптимизации основанной на байесовской модели, чтобы попытаться понять, как работает байесовский метод и какие преимущества он имеет по сравнению со случайными методами поиска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt\n",
    "\n",
    "Hyperopt - одна из нескольких библиотек автоматического подбора гиперпараметров, использующих байесовскую оптимизацию. Эти библиотеки различаются по алгоритму, используемому как для создания surrogate (вероятностной модели) целевой функции, так и для выбора следующих гиперпараметров для оценки в целевой функции. Hyperopt использует Tree Parzen Estimator (TPE). Другие библиотеки Python включают Spearmint, который использует гауссовский процесс для surrogate, и SMAC, который использует регрессию случайного леса.\n",
    "\n",
    "Hyperopt имеет простой синтаксис для структурирования задачи оптимизации, распространяющийся на настройку гиперпараметров для любой проблемы, включающей в себя минимизацию функции. Более того, структура задачи байесовской оптимизации одинакова во всех библиотеках, с основными различиями в синтаксисе (и в алгоритмах под капотом, с которыми мы не будем иметь дело)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "MAX_EVALS = 500\n",
    "N_FOLDS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "В этом ноутбуке мы будем работать с датасетом Caravan Insurance Challenge  [взятым с Kaggle](https://www.kaggle.com/uciml/caravan-insurance-challenge). Обучая модель на прошлых данных, нужно определить, будет ли потенциальный клиент покупать страховой полис. Это простая задача классификации машинного обучения с учителем: мы хотим обучить модель на прошлых данных, для прогнозирования бинарного результата на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (5822, 85)\n",
      "Test shape:  (4000, 85)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>ALEVEN</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  MOSHOOFD  MGODRK  MGODPR  MGODOV  \\\n",
       "0       33         1        3         2         8       0       5       1   \n",
       "1       37         1        2         2         8       1       4       1   \n",
       "2       37         1        2         2         8       0       4       2   \n",
       "3        9         1        3         3         3       2       3       2   \n",
       "4       40         1        4         2        10       1       4       1   \n",
       "\n",
       "   MGODGE  MRELGE  ...  ALEVEN  APERSONG  AGEZONG  AWAOREG  ABRAND  AZEILPL  \\\n",
       "0       3       7  ...       0         0        0        0       1        0   \n",
       "1       4       6  ...       0         0        0        0       1        0   \n",
       "2       4       3  ...       0         0        0        0       1        0   \n",
       "3       4       5  ...       0         0        0        0       1        0   \n",
       "4       4       7  ...       0         0        0        0       1        0   \n",
       "\n",
       "   APLEZIER  AFIETS  AINBOED  ABYSTAND  \n",
       "0         0       0        0         0  \n",
       "1         0       0        0         0  \n",
       "2         0       0        0         0  \n",
       "3         0       0        0         0  \n",
       "4         0       0        0         0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and separate into training and testing sets\n",
    "data = pd.read_csv('data/caravan-insurance-challenge.csv')\n",
    "train = data[data['ORIGIN'] == 'train']\n",
    "test = data[data['ORIGIN'] == 'test']\n",
    "\n",
    "# Extract the labels and format properly\n",
    "train_labels = np.array(train['CARAVAN'].astype(np.int32)).reshape((-1,))\n",
    "test_labels = np.array(test['CARAVAN'].astype(np.int32)).reshape((-1,))\n",
    "\n",
    "# Drop the unneeded columns\n",
    "train = train.drop(columns = ['ORIGIN', 'CARAVAN'])\n",
    "test = test.drop(columns = ['ORIGIN', 'CARAVAN'])\n",
    "\n",
    "# Convert to numpy array for splitting in cross validation\n",
    "features = np.array(train)\n",
    "test_features = np.array(test)\n",
    "labels = train_labels[:]\n",
    "\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распределение меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(labels, edgecolor = 'k'); \n",
    "plt.xlabel('Label'); plt.ylabel('Count'); plt.title('Counts of Labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это задача с несбалансированными классами: гораздо больше наблюдений, когда страховой полис не был куплен (0), чем когда он был куплен (1). Следовательно, точность - плохая метрика для этой задачи. Вместо этого мы будем использовать обычную кассификационную метрику ROC AUC. Случайное угадывание в задаче классификации даст ROC AUC 0.5, а идеальный классификатор даст ROC AUC 1.0. Для лучшей базовой модели, чем случайное угадывание, мы можем обучить Gradient Boosting Machine по умолчанию и делать прогнозы с ее помощью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель по умолчанию Gradient Boosting Machine\n",
    "\n",
    "Мы будем использовать LightGBM-реализацию gradient boosting machine. Это намного быстрее, чем реализация Scikit-Learn, и дает результаты, сравнимые с extreme gradient boosting, XGBoost. Для базовой модели мы будем использовать гиперпараметры по умолчанию,  указанные в LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with default hyperparameters\n",
    "model = lgb.LGBMClassifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все, что нам нужно сделать, это обучить модель на тренировочных данных и сделать прогнозы на тестовых. Поскольку мы измеряем ROC AUC, а не точность, мы будем использовать для прогнозов предсказанные моделью вероятности, а не жесткие двоичные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат базовой модели на тестовых данных равен 0.7092.\n",
      "Время обучения тестовой модели равно 0.2011 секунды\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "model.fit(features, labels)\n",
    "train_time = timer() - start\n",
    "\n",
    "predictions = model.predict_proba(test_features)[:, 1]\n",
    "auc = roc_auc_score(test_labels, predictions)\n",
    "\n",
    "print('Результат базовой модели на тестовых данных равен {:.4f}.'.format(auc))\n",
    "print('Время обучения тестовой модели равно {:.4f} секунды'.format(train_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это метрики которые нам нужно улучшить. Из-за небольшого размера набора данных (менее 6000 наблюдений) настройка гиперпараметров окажет умеренное, но заметное влияние на качество модели (большего эффекта можно добиться собрав больше данных!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайный поиск\n",
    "\n",
    "Сначала мы реализуем обычный метод оптимизации гиперпараметров: случайный поиск. На каждой итерации мы выбираем случайный набор гиперпараметров модели из пространства поиска. Эмпирически случайный поиск очень эффективен, он дает близкие к поиску по сетке результаты, при этом значительно сокращая время, затрачиваемое на поиск. Однако это все еще неинформированный метод в том смысле, что он не использует прошлые оценки целевой функции для информирования при выборе, который он делает для следующей оценки.\n",
    "\n",
    "Случайный поиск состоит из следующих четырех частей, которые также используются в байесовской оптимизации гиперпараметров:\n",
    "\n",
    "1. Область определения: диапазоны значений в которых происходит поиск\n",
    "2. Алгоритм оптимизации: случайно выбираем следующее значение! (да это квалифицируется как алгоритм)\n",
    "3. Минимизируемая целевая функция: в нашем случае это кросс валидация ROC AUC\n",
    "4. История результатов которая отслеживает попытки выбора гиперпараметров и кросс-валидационную метрику.\n",
    "\n",
    "Случайный поиск может быть реализован в библиотеке Scikit-Learn с использованием `RandomizedSearchCV`, однако, поскольку мы используем раннюю остановку (для определения оптимального числа оценщиков), мы реализуем этот метод самостоятельно (больше практики!). Это довольно просто, и многие идеи случайного поиска будут использованы в байесовской оптимизации гиперпараметров. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Область определения для случайного поиска\n",
    "\n",
    "И случайный поиск и байесовкая опимизация ищут гиперпараметры в некоторой области. Для случайного (или поиска по сетке) эта область называется сетка гиперпараметров и использует дискретные значения для гиперпараметров.\n",
    "\n",
    "Сперва взглянем на все гиперпараметры которые нужно настроить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе значений по умолчанию мы можем построить следующую сетку гиперпараметров. Сложно сказать заранее, какой выбор будет работать лучше, поэтому мы будем использовать широкий диапазон значений центрированный относительно значения по умолчанию для большинства гиперпараметров.\n",
    "\n",
    "`subsample_dist` будет использован для `subsample` параметра, но мы не можем добавить его в сетку параметров поскольку `boosting_type=goss` не поддерживает подвыборку строк. Поэтому мы будем использовать оператор `if` при выборе наших гиперпараметров, чтобы выбрать пропорцию подвыборки если тип бустинга не  `goss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(30, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log(0.005), np.log(0.2), base = np.exp(1), num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10))\n",
    "}\n",
    "\n",
    "# Subsampling (only applicable with 'goss')\n",
    "subsample_dist = list(np.linspace(0.5, 1, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим два распределения: learning_rate и num_leaves. Скорость обучения обычно [представлена логарифмическим распределением](https://www.quora.com/Why-does-one-sample-the-log-space-when-searching-for-good-Hyper-Parameters-for-Machine-Learning), поскольку она может варьироваться в пределах нескольких порядков. np.logspace возвращает значения, равномерно распределенные по логарифмической шкале (поэтому, если мы возьмем логарифм результатов, распределение будет равномерным)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(param_grid['learning_rate'], color = 'r', edgecolor = 'k');\n",
    "plt.xlabel('Learning Rate', size = 14); plt.ylabel('Count', size = 14); plt.title('Learning Rate Distribution', size = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Меньшие значения скорости обучения чаще встречаются со значениями от 0,005 до 0,2. Диапазон области довольно велик, что указывает на большую неопределенность с нашей стороны относительно оптимального значения (которое, мы надеемся, находится где-то в сетке)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(param_grid['num_leaves'], color = 'm', edgecolor = 'k')\n",
    "plt.xlabel('Learning Number of Leaves', size = 14); plt.ylabel('Count', size = 14); plt.title('Number of Leaves Distribution', size = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество листьев это обычная равномерная область\n",
    "\n",
    "### Выборка из области определения гиперпараметра\n",
    "\n",
    "Посмотрим как мы делаем выборку из множества гиперпараметров из нашей сетки используя понимание словаря (dictionary comprehension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'boosting_type': 'dart',\n",
       " 'num_leaves': 35,\n",
       " 'learning_rate': 0.015082225756147042,\n",
       " 'subsample_for_bin': 60000,\n",
       " 'min_child_samples': 285,\n",
       " 'reg_alpha': 0.7346938775510203,\n",
       " 'reg_lambda': 0.2040816326530612,\n",
       " 'colsample_bytree': 0.6}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sample parameters for gbm\n",
    "params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы добавить долю `subsample` если `boosting_type` не `goss`, мы можем использовать оператор if."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'boosting_type': 'dart',\n",
       " 'num_leaves': 35,\n",
       " 'learning_rate': 0.015082225756147042,\n",
       " 'subsample_for_bin': 60000,\n",
       " 'min_child_samples': 285,\n",
       " 'reg_alpha': 0.7346938775510203,\n",
       " 'reg_lambda': 0.2040816326530612,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'subsample': 0.6666666666666667}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['subsample'] = random.sample(subsample_dist, 1)[0] if params['boosting_type'] != 'goss' else 1.0\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы установим значение subsample равным 1.0 если вид бустинга - goss что значит что мы не используем подвыборки. (При использовании подвыборки мы обучаемся на подмножестве строк (наблюдений) а не на всем множестве данных. Эта техника также называется bagging для \"bootstrap aggregating\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-валидация с ранней остановкой в LightGBM\n",
    "\n",
    "Кросс-валидация api scikit-learn не имеет опцию ранней остановки. Поэтому мы будем использовать функцию кросс-валидации LightGBM с 100 раундов ранней остановки. Для использования этой функции нам нужно создать датасет из наших признаков и меток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание lgb датасета\n",
    "train_set = lgb.Dataset(features, label = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `cv` принимает параметры, обучающие данные, количество раундов обучения, количество фолдов, метрику, количество ранних остановок и несколько других аргументов. Мы установили количество раундов бустинга очень высоким, но на самом деле мы не будем обучать так много оценщиков, потому что мы используем раннюю остановку и прекращаем обучение, когда оценка валидации не улучшилась для 100 оценщиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальный ROC AUC на проверочных данных был 0.76462 с std равным 0.03837.\n",
      "Идеальное число итераций было 309.\n"
     ]
    }
   ],
   "source": [
    "# Выполняет кросс-валидацию с 10 фолдами\n",
    "r = lgb.cv(params, train_set, num_boost_round = 10000, nfold = 10, metrics = 'auc', \n",
    "           early_stopping_rounds = 100, verbose_eval = False, seed = 50)\n",
    "\n",
    "# Наибольший результат\n",
    "r_best = np.max(r['auc-mean'])\n",
    "\n",
    "# Стандартное отклонение наилучего результата\n",
    "r_best_std = r['auc-stdv'][np.argmax(r['auc-mean'])]\n",
    "\n",
    "print('Максимальный ROC AUC на проверочных данных был {:.5f} с std равным {:.5f}.'.format(r_best, r_best_std))\n",
    "print('Идеальное число итераций было {}.'.format(np.argmax(r['auc-mean']) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датафрейм результатов\n",
    "\n",
    "У нас есть наши область и алгоритм, который в данном случае является случайным выбором. Две другие части, которые нам нужны для задачи оптимизации, - это целевая функция и структура данных для отслеживания результатов (эти же четыре части потребуются при байесовской оптимизации).\n",
    "\n",
    "Отслеживание результатов будет осуществляться в `dataframe`, где в каждой строке будет храниться одна оценка целевой функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to hold cv results\n",
    "random_results = pd.DataFrame(columns = ['loss', 'params', 'iteration', 'estimators', 'time'],\n",
    "                       index = list(range(MAX_EVALS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Целевая функция\n",
    "\n",
    "Целевая функция примет гиперпараметры и вернет потери на проверочных данных (вместе с некоторой другой информацией для отслеживания прогресса поиска). Мы уже выбрали в качестве метрики ROC AUC, и теперь нам нужно выяснить, как его измерить. Мы не можем оценить ROC AUC на тестовом наборе, потому что это будет обманом. Вместо этого мы должны использовать проверочный набор данных для настройки модели и надеяться, что результаты распространятся и на тестовое множество.\n",
    "\n",
    "Вместо выделения проверочных данных из обучающей выборки (что ограничивает количество имеющихся у нас данных обучения), лучше использовать перекрестную проверку KFold. Помимо того что мы не уменьшаем обучающие данные, этот метод должен также дать нам лучшую оценку ошибки обобщения на тестовые данные, потому что мы будем использовать K проверок, а не только одну. В этом примере мы будем использовать 10-фолдовую кросс-валидацию, что означает тестирование и обучение каждого набора гиперпараметров модели 10 раз, каждый раз используя различные подмножества обучающих данных в качестве проверочной выборки. Целевая функция вернет список информации, включая валидационный AUC ROC. Мы также хотим сохранить используемые гиперпараметры, чтобы мы знали, какие из них являются оптимальными (или лучшими из тех, что мы пробовали).\n",
    "\n",
    "В случае случайного поиска следующие выбранные значения не зависят от результатов прошлой оценки, но мы должны четко отслеживать их, чтобы знать, какие значения сработали лучше всего! Это также позволит нам сопоставить случайный поиск с информированной байесовской оптимизацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_objective(params, iteration, n_folds = N_FOLDS):\n",
    "    \"\"\"Random search objective function. Takes in hyperparameters\n",
    "       and returns a list of results to be saved.\"\"\"\n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Выполняет n_folds кросс-валидацию\n",
    "    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "    end = timer()\n",
    "    best_score = np.max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Потери должны быть минимизированы\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Раунды бустинга возвращающие наибольший cv score\n",
    "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "    \n",
    "    # Возвращение списка результатов\n",
    "    return [loss, params, iteration, n_estimators, end - start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Имплементация случайного поиска\n",
    "\n",
    "Теперь мы можем написать цикл для итерации по количеству оценок, каждый раз выбирая для оценки различный набор гиперпараметров. При каждом прохождении функции результаты сохраняются в датафрейм. (Магия `%% capture` перехватывает любые выходные данные из ячейки в блокноте Jupyter. Это полезно, потому что выходные данные обучающего прогона LightGBM не могут быть заглушены).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 76, 'learning_rate': 0.055533968216953146, 'subsample_for_bin': 80000, 'min_child_samples': 460, 'reg_alpha': 0.6122448979591836, 'reg_lambda': 1.0, 'colsample_bytree': 0.8222222222222222}\n",
      "{'class_weight': None, 'boosting_type': 'dart', 'num_leaves': 70, 'learning_rate': 0.011646863384981408, 'subsample_for_bin': 220000, 'min_child_samples': 375, 'reg_alpha': 0.1020408163265306, 'reg_lambda': 0.18367346938775508, 'colsample_bytree': 0.8222222222222222}\n",
      "{'class_weight': 'balanced', 'boosting_type': 'goss', 'num_leaves': 58, 'learning_rate': 0.010197109660117238, 'subsample_for_bin': 40000, 'min_child_samples': 230, 'reg_alpha': 0.7755102040816326, 'reg_lambda': 0.7755102040816326, 'colsample_bytree': 0.8666666666666667}\n",
      "{'class_weight': None, 'boosting_type': 'gbdt', 'num_leaves': 118, 'learning_rate': 0.048981732603349946, 'subsample_for_bin': 280000, 'min_child_samples': 230, 'reg_alpha': 0.8979591836734693, 'reg_lambda': 0.836734693877551, 'colsample_bytree': 0.8666666666666667}\n",
      "{'class_weight': None, 'boosting_type': 'goss', 'num_leaves': 64, 'learning_rate': 0.10213232309588141, 'subsample_for_bin': 140000, 'min_child_samples': 465, 'reg_alpha': 0.12244897959183673, 'reg_lambda': 0.24489795918367346, 'colsample_bytree': 0.9111111111111111}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9bb1341efb38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresults_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Add results to next row in dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-114433e75592>\u001b[0m in \u001b[0;36mrandom_objective\u001b[0;34m(params, iteration, n_folds)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Выполняет n_folds кросс-валидацию\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n\u001b[0;32m----> 9\u001b[0;31m                         early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auc-mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[1;32m    481\u001b[0m                                     \u001b[0mend_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mcvfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_agg_cv_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mhandlerFunction\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboosters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandlerFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1753\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1754\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1755\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1756\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1757\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "random.seed(50)\n",
    "\n",
    "# Iterate through the specified number of evaluations\n",
    "for i in range(MAX_EVALS):\n",
    "    \n",
    "    # Randomly sample parameters for gbm\n",
    "    params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    if params['boosting_type'] == 'goss':\n",
    "        # Cannot subsample with goss\n",
    "        params['subsample'] = 1.0\n",
    "    else:\n",
    "        # Subsample supported for gdbt and dart\n",
    "        params['subsample'] = random.sample(subsample_dist, 1)[0]\n",
    "        \n",
    "        \n",
    "    results_list = random_objective(params, i)\n",
    "    \n",
    "    # Add results to next row in dataframe\n",
    "    random_results.loc[i, :] = results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>iteration</th>\n",
       "      <th>estimators</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.232707</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>292</td>\n",
       "      <td>161</td>\n",
       "      <td>5.35296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.233449</td>\n",
       "      <td>{'class_weight': None, 'boosting_type': 'dart'...</td>\n",
       "      <td>134</td>\n",
       "      <td>166</td>\n",
       "      <td>5.02008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.234382</td>\n",
       "      <td>{'class_weight': None, 'boosting_type': 'gbdt'...</td>\n",
       "      <td>369</td>\n",
       "      <td>295</td>\n",
       "      <td>5.29839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.23443</td>\n",
       "      <td>{'class_weight': None, 'boosting_type': 'gbdt'...</td>\n",
       "      <td>146</td>\n",
       "      <td>455</td>\n",
       "      <td>7.01225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.234836</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>284</td>\n",
       "      <td>115</td>\n",
       "      <td>4.52999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params iteration  \\\n",
       "0  0.232707  {'class_weight': 'balanced', 'boosting_type': ...       292   \n",
       "1  0.233449  {'class_weight': None, 'boosting_type': 'dart'...       134   \n",
       "2  0.234382  {'class_weight': None, 'boosting_type': 'gbdt'...       369   \n",
       "3   0.23443  {'class_weight': None, 'boosting_type': 'gbdt'...       146   \n",
       "4  0.234836  {'class_weight': 'balanced', 'boosting_type': ...       284   \n",
       "\n",
       "  estimators     time  \n",
       "0        161  5.35296  \n",
       "1        166  5.02008  \n",
       "2        295  5.29839  \n",
       "3        455  7.01225  \n",
       "4        115  4.52999  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort results by best validation score\n",
    "random_results.sort_values('loss', ascending = True, inplace = True)\n",
    "random_results.reset_index(inplace = True, drop = True)\n",
    "random_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эффективность случайного поиска\n",
    "\n",
    "Напомним, что базовая модель градиентного бустинга достигла результата 0.71 на тренировочных данных. Мы можем использовать лучшие параметры для случайного поиска и оценить их на тестовом множестве. \n",
    "\n",
    "Чему равны гиперпараметры вернувшие наибольший результат на целевой функции??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'boosting_type': 'goss',\n",
       " 'num_leaves': 76,\n",
       " 'learning_rate': 0.055533968216953146,\n",
       " 'subsample_for_bin': 80000,\n",
       " 'min_child_samples': 460,\n",
       " 'reg_alpha': 0.6122448979591836,\n",
       " 'reg_lambda': 1.0,\n",
       " 'colsample_bytree': 0.8222222222222222,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_results.loc[0, 'params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключ `estimators` содержит среднеее число оценщиков обученных с ранней остановкой (усредненные относительно 10 фолдов). Мы можем использовать это как оптимальное число оценщиков в модели градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая модель из результатов случайного поиска на тестовых данных 0.7108.\n",
      "Это было достигнуто используя 0 поисковых итераций.\n"
     ]
    }
   ],
   "source": [
    "# Найдем лучшие параметры и число оценщиков\n",
    "best_random_params = random_results.loc[0, 'params'].copy()\n",
    "best_random_estimators = int(random_results.loc[0, 'estimators'])\n",
    "best_random_model = lgb.LGBMClassifier(n_estimators=best_random_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', **best_random_params, random_state = 50)\n",
    "\n",
    "# Обучимся на тренировочных данных\n",
    "best_random_model.fit(features, labels)\n",
    "\n",
    "# Сделаем предсказания на тестовых данных\n",
    "predictions = best_random_model.predict_proba(test_features)[:, 1]\n",
    "\n",
    "\n",
    "print('Лучшая модель из результатов случайного поиска на тестовых данных {:.4f}.'.format(roc_auc_score(test_labels, predictions)))\n",
    "print('Это было достигнуто используя {} поисковых итераций.'.format(random_results.loc[0, 'iteration']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скромное улучшение по сравнению с базовым уровнем. Использование большего количества оценок может увеличить результат, но за счет увеличения времени оптимизации. Мы также должны помнить, что гиперпараметры оптимизированы на проверочных данных, что может не сработать на тестовых.\n",
    "\n",
    "Сейчас мы можем перейти к байесовским методам и посмотреть можно ли на них получить лучшие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Байесовская оптимизация гиперпараметров с использованием Hyperopt\n",
    "\n",
    "Для байесовской оптимизации нам нужны следующие 4 пункта:\n",
    "\n",
    "1. Целевая функция\n",
    "2. Пространство определения значений\n",
    "3. Алгоритм оптимизации гиперпараметров\n",
    "4. История результатов\n",
    "\n",
    "Мы уже использовали все это в случайном поиске, но для Hyperopt мы сделаем несколько изменений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Целевая функция\n",
    "\n",
    "Эта целевая функция все еще принимает на вход гиперпараметры, но она возвращает не список, а словарь. Единственное требование для целевой функции в Hyperopt это то, что она должна иметь в возвращаемом словаре ключ `\"loss\"`  для минимизации и ключ называемый `\"status\"` указывающий на то, что выполнение было успешно.\n",
    "\n",
    "Если мы хотим отслеживать число итераций, мы можем объявить глобальную переменную `ITERATION` которая увеличивается при каждом вызове функции. Кроме того для получения подробных результатов, каждый раз когда выполняется функция, мы будем писать результаты в новую строку csv файла. Это может быть полезно для очень долгих вычислений, если мы хотим проверять прогресс (это может быть не самое элегантное решение, но оно лучше чем выводить результаты в консоль, поскольку в нашем случае мы их сохраняем!) \n",
    "\n",
    "Наиболее важной частью этой функции является то, что нам сейчас нужно вернуть __value to minimize__ а не сырой ROC AUC. Мы пытаемся найти лучшее значение целевой функции, и хоть больший ROC AUC лучше, Hyperopt может только минимизировать функцию. Поэтому простое решение - возвращать 1 - ROC (мы делали это и для случайного поиска в качестве практики)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from hyperopt import STATUS_OK\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def objective(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Целевая функция для оптимизации гиперпараметров Gradient Boosting Machine\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Получение подвыборки если небоходимо, иначе установка 1.0\n",
    "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Получение типа бустинга\n",
    "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    # Убедимся что параметры которые должны быть целочисленны - целочисленны\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Выполним n_folds кросс-валидацию\n",
    "    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Получим лучший результат\n",
    "    best_score = np.max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Потери должны быть минимизированы\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Раунды бустинга возвращающие наибольший cv score\n",
    "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "\n",
    "    # Запишем в csv ('a' значит append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n",
    "    \n",
    "    # Словарь с информацией для выполнения\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'estimators': n_estimators, \n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя Hyperopt-у нужен только `loss`, хорошо бы отслеживать и другие метрики, чтобы мы могли проверить результаты. Позже мы можем сравнить последовательности поисков с двух подходов, что поможет нам понять как работает метод. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Space\n",
    "\n",
    "Specifying the domain (called the `space` in Hyperopt) is a little trickier than in grid search. In Hyperopt, and other Bayesian optimization frameworks, the domian is not a discrete grid but instead has probability distributions for each hyperparameter. For each hyperparameter, we will use the same limits as with the grid, but instead of being defined at each point, the domain represents probabilities for each hyperparameter. This will probably become clearer in the code and the images! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will go through an example of the learning rate. Again, we are using a log-uniform space for the learning rate defined from 0.005 to 0.2 (same as with the grid from random search.) This time, when we graph the domain, it's more accurate to see a kernel density estimate plot than a histogram (although both show distributions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the learning rate\n",
    "learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the learning rate by sampling from the space using a Hyperopt utility. Here we plot 10000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_dist = []\n",
    "\n",
    "# Draw 10000 samples from the learning rate domain\n",
    "for _ in range(10000):\n",
    "    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n",
    "    \n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.kdeplot(learning_rate_dist, color = 'red', linewidth = 2, shade = True);\n",
    "plt.title('Learning Rate Distribution', size = 18); \n",
    "plt.xlabel('Learning Rate', size = 16); plt.ylabel('Density', size = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of leaves is again a uniform distribution. Here we used `quniform` which means a discrete uniform (as opposed to continuous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete uniform distribution\n",
    "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
    "num_leaves_dist = []\n",
    "\n",
    "# Sample 10000 times from the number of leaves distribution\n",
    "for _ in range(10000):\n",
    "    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n",
    "    \n",
    "# kdeplot\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.kdeplot(num_leaves_dist, linewidth = 2, shade = True);\n",
    "plt.title('Number of Leaves Distribution', size = 18); plt.xlabel('Number of Leaves', size = 16); plt.ylabel('Density', size = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Domain\n",
    "\n",
    "In Hyperopt, we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters. For example, we know that `goss` boosting type cannot use subsample, so when we set up the `boosting_type` categorical variable, we have to se the `subsample` to 1.0 while for the other boosting types it's a float between 0.5 and 1.0 Let's see this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosting type domain \n",
    "boosting_type = {'boosting_type': hp.choice('boosting_type', \n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n",
    "                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n",
    "\n",
    "# Draw a sample\n",
    "params = sample(boosting_type)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set both the `boosting_type` and `subsample` as top-level keys in the parameter dictionary. We can use the Python `dict.get` method with a default value of 1.0. This means that if the key is not present in the dictionary, the value returned will be the default (1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the subsample if present otherwise set to 1.0\n",
    "subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "\n",
    "# Extract the boosting type\n",
    "params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "params['subsample'] = subsample\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the gbm cannot use the nested dictionary so we need to set the `boosting_type` and `subsample` as top level keys. Nested conditionals allow us to use a different set of hyperparameters depending on other hyperparameters. For example, we can explore different models with completely different sets of hyperparameters by using nested conditionals. The only requirement is that the first nested statement must be based on a `choice` hyperparameter (the choice could be the type of model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Bayesian Domain\n",
    "\n",
    "Now we can define the entire domain. Each variable needs to have a label and a few parameters specifying the type and extent of the distribution. For the variables such as boosting type that are categorical, we use the `choice` variable. Other variables types include `quniform`, `loguniform`, and `uniform`. For the complete list, check out the [documentation](https://github.com/hyperopt/hyperopt/wiki/FMin) for Hyperopt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "space = {\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Sampling from the Domain \n",
    "\n",
    "Let's sample from the domain (using the conditional logic) to see the result of each draw. Every time we run this code, the results will change. (Again notice that we need to assign the top level keys to the keywords understood by the GBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the full space\n",
    "x = sample(space)\n",
    "\n",
    "# Conditional logic to assign top-level keys\n",
    "subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "x['subsample'] = subsample\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample(space)\n",
    "subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "x['subsample'] = subsample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithm\n",
    "\n",
    "Although this is the most technical part of Bayesian optimization, defining the algorithm to use in Hyperopt is simple. We will use the Tree Parzen Estimator (read about it [in this paper](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)) which is one method for constructing the surrogate function and choosing the next hyperparameters to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe\n",
    "\n",
    "# optimization algorithm\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results History\n",
    "\n",
    "The final part is the result history. Here, we are using two methods to make sure we capture all the results:\n",
    "\n",
    "1. A `Trials` object that stores the dictionary returned from the objective function\n",
    "2. Writing to a csv file every iteration\n",
    "\n",
    "The csv file option also lets us monitor the results of an on-going experiment. (Although do not use Excel to open the file while training is on-going. Instead check the results using `tail results/gbm_trials.csv` from bash or another command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "\n",
    "# Keep track of results\n",
    "bayes_trials = Trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Trials` object will hold everything returned from the objective function in the `.results` attribute. It also holds other information from the search, but we return everything we need from the objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to save first results\n",
    "out_file = 'results/gbm_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time the objective function is called, it will write one line to this file. Running the cell above does clear the file though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "\n",
    "We have everything in place needed to run the optimization. First we declare the global variable that will be used to keep track of the number of iterations. Then, we call `fmin` passing in everything we defined above and the maximum number of iterations to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Global variable\n",
    "global  ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.results` attribute of the `Trials` object has all information from the objective function. If we sort this by the lowest loss, we can see the hyperparameters that performed the best in terms of validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the results from the csv file (which might be easier since it's already a dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results/gbm_trials.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, when we save to a file and then read back in, the dictionary of hyperparameters is represented as a string. To convert from a string back to a dictionary we can use the `ast` library and the `literal_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Best Results\n",
    "\n",
    "Now for the moment of truth: did the optimization pay off? For this problem with a relatively small dataset, the benefits of hyperparameter optimization compared to random search are probably minor (if there are any). Random search might turn up a better result in fewer iterations simply becuase of randomness! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "best_bayes_model.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the testing data \n",
    "preds = best_bayes_model.predict_proba(test_features)[:, 1]\n",
    "print('The best model from Bayes optimization scores {:.5f} AUC ROC on the test set.'.format(roc_auc_score(test_labels, preds)))\n",
    "print('This was achieved after {} search iterations'.format(results.loc[0, 'iteration']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes Optimization scored slightly higher on the test data ROC AUC (here unlike the loss, higher is better) but also took more iterations to reach the best score (if the notebook is re-run, the results may change). The Bayesian Optimization also does better in terms of the validation loss (1 - ROC AUC) scoring 0.229 compared to 0.231. Due to the small differences, it's hard to say that Bayesian Optimization is better for this particular problem. As with any other machine learning technique, the effectiveness of Bayesian Optimization will be problem dependent. For this problem, we see a slight benefit but it is also possible that random search may find a better set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to Random Search\n",
    "\n",
    "Comparing the results to random seach both in numbers and figures can help us understand how Bayesian Optimization searches work. First, we can look at the best hyperparameters (as determined from the validation error) from both searches.\n",
    "\n",
    "### Optimal Hyperparameters\n",
    "\n",
    "We can compare the \"best\" hyperparameters found from both search methods. It's interesting to compare the results because they suggest there may be multiple configurations that yield roughly the same validation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_params['method'] = 'random search'\n",
    "best_bayes_params['method'] = 'Bayesian optimization'\n",
    "best_params = pd.DataFrame(best_bayes_params, index = [0]).append(pd.DataFrame(best_random_params, index = [0]), \n",
    "                                                                  ignore_index = True, sort = True)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top row is the Bayesian Optimization and the bottom row is random search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Hyperparameters\n",
    "\n",
    "One interesting thing we can do with the results is to see the different hyperparameters tried by both random search and the Tree Parzen Estimator. Since random search is choosing without regards to the previous results, we would expect that the distribution of samples should be close to the domain space we defined (it won't be exact since we are using a fairly small number of iterations). On the other hand, the Bayes Optimization, if given enough time, should concetrate on the \"more promising\" hyperparameters. \n",
    "\n",
    "In addition to a more concentrated search, we expect that the average validation loss of the Bayesian Optimization should be lower than that on the random search because it chooses values likely (according to the probability model) to yield lower losses on the objective function. The validation loss should also decrease over time with the Bayesian method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need to extract the hyperparameters from both search methods. We will store these in separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for storing parameters\n",
    "random_params = pd.DataFrame(columns = list(random_results.loc[0, 'params'].keys()),\n",
    "                            index = list(range(len(random_results))))\n",
    "\n",
    "# Add the results with each parameter a different column\n",
    "for i, params in enumerate(random_results['params']):\n",
    "    random_params.loc[i, :] = list(params.values())\n",
    "    \n",
    "random_params['loss'] = random_results['loss']\n",
    "random_params['iteration'] = random_results['iteration']\n",
    "random_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for storing parameters\n",
    "bayes_params = pd.DataFrame(columns = list(ast.literal_eval(results.loc[0, 'params']).keys()),\n",
    "                            index = list(range(len(results))))\n",
    "\n",
    "# Add the results with each parameter a different column\n",
    "for i, params in enumerate(results['params']):\n",
    "    bayes_params.loc[i, :] = list(ast.literal_eval(params).values())\n",
    "    \n",
    "bayes_params['loss'] = results['loss']\n",
    "bayes_params['iteration'] = results['iteration']\n",
    "\n",
    "bayes_params.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rates\n",
    "\n",
    "The first plot shows the sampling distribution, random search, and Bayesian optimization learning rate distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 8))\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "# Density plots of the learning rate distributions \n",
    "sns.kdeplot(learning_rate_dist, label = 'Sampling Distribution', linewidth = 2)\n",
    "sns.kdeplot(random_params['learning_rate'], label = 'Random Search', linewidth = 2)\n",
    "sns.kdeplot(bayes_params['learning_rate'], label = 'Bayes Optimization', linewidth = 2)\n",
    "plt.legend()\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('Density'); plt.title('Learning Rate Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Type \n",
    "\n",
    "Random search should use the boosting types with the same frequency. However, Bayesian Optimization might have decided (modeled) that one boosting type is better than another for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey = True, sharex = True)\n",
    "\n",
    "# Bar plots of boosting type\n",
    "random_params['boosting_type'].value_counts().plot.bar(ax = axs[0], figsize = (14, 6), color = 'orange', title = 'Random Search Boosting Type')\n",
    "bayes_params['boosting_type'].value_counts().plot.bar(ax = axs[1], figsize = (14, 6), color = 'green', title = 'Bayes Optimization Boosting Type');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Search boosting type percentages')\n",
    "100 * random_params['boosting_type'].value_counts() / len(random_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bayes Optimization boosting type percentages')\n",
    "100 * bayes_params['boosting_type'].value_counts() / len(bayes_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, the Bayesian Optimization tried the gradient boosted decision tree boosting type much more than the other two. We could use this information to inform subsequent searches for the best hyperparameters by focusing on a smaller domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of All Numeric Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate through each hyperparameter\n",
    "for i, hyper in enumerate(random_params.columns):\n",
    "    if hyper not in ['class_weight', 'boosting_type', 'iteration', 'subsample', 'metric', 'verbose']:\n",
    "        plt.figure(figsize = (14, 6))\n",
    "        # Plot the random search distribution and the bayes search distribution\n",
    "        if hyper != 'loss':\n",
    "            sns.kdeplot([sample(space[hyper]) for _ in range(1000)], label = 'Sampling Distribution')\n",
    "        sns.kdeplot(random_params[hyper], label = 'Random Search')\n",
    "        sns.kdeplot(bayes_params[hyper], label = 'Bayes Optimization')\n",
    "        plt.legend(loc = 1)\n",
    "        plt.title('{} Distribution'.format(hyper))\n",
    "        plt.xlabel('{}'.format(hyper)); plt.ylabel('Density');\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final graph shows that the validation loss for Bayesian Optimization tends to be lower than than from Random Search. This should give us confidence the method is working correctly. Again, this does not mean the hyperparameters found during Bayesian Optimization are necessarily better for the test set, only that they yield a lower loss in cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of Hyperparameters Searched\n",
    "\n",
    "We can also plot the hyperparameters over time (against the number of iterations) to see how they change for the Bayes Optimization. First we will map the `boosting_type` to an integer for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map boosting type to integer (essentially label encoding)\n",
    "bayes_params['boosting_int'] = bayes_params['boosting_type'].replace({'gbdt': 1, 'goss': 2, 'dart': 3})\n",
    "\n",
    "# Plot the boosting type over the search\n",
    "plt.plot(bayes_params['iteration'], bayes_params['boosting_int'], 'ro')\n",
    "plt.yticks([1, 2, 3], ['gdbt', 'goss', 'dart']);\n",
    "plt.xlabel('Iteration'); plt.title('Boosting Type over Search');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much change over time for this hyperparameter: `gdbt` is dominant for the entire stretch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize = (24, 6))\n",
    "i = 0\n",
    "\n",
    "# Plot of four hyperparameters\n",
    "for i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n",
    "    \n",
    "        # Scatterplot\n",
    "        sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n",
    "        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize = (18, 6))\n",
    "i = 0\n",
    "\n",
    "# Scatterplot of next three hyperparameters\n",
    "for i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin']):\n",
    "        sns.regplot('iteration', hyper, data = bayes_params, ax = axs[i])\n",
    "        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are trends in these plots, we can use them to inform subsequent searches. We might even want to use grid search focusing on a much smaller region of hyperparameter space based on the Bayesian Optimization results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Losses\n",
    "\n",
    "Finally, we can look at the losses recorded by both random search and Bayes Optimization. We would expect the average loss recorded by Bayes Optimization to be lower because this method should spend more time in promising regions of the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of just scores\n",
    "scores = pd.DataFrame({'ROC AUC': 1 - random_params['loss'], 'iteration': random_params['iteration'], 'search': 'random'})\n",
    "scores = scores.append(pd.DataFrame({'ROC AUC': 1 - bayes_params['loss'], 'iteration': bayes_params['iteration'], 'search': 'Bayes'}))\n",
    "\n",
    "scores['ROC AUC'] = scores['ROC AUC'].astype(np.float32)\n",
    "scores['iteration'] = scores['iteration'].astype(np.int32)\n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make histograms of the scores (not taking in account the iteration) on the same x-axis scale to see if there is a difference in scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 6))\n",
    "\n",
    "# Random search scores\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(1 - random_results['loss'].astype(np.float64), label = 'Random Search', edgecolor = 'k');\n",
    "plt.xlabel(\"Validation ROC AUC\"); plt.ylabel(\"Count\"); plt.title(\"Random Search Validation Scores\")\n",
    "plt.xlim(0.75, 0.78)\n",
    "\n",
    "# Bayes optimization scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(1 - bayes_params['loss'], label = 'Bayes Optimization', edgecolor = 'k');\n",
    "plt.xlabel(\"Validation ROC AUC\"); plt.ylabel(\"Count\"); plt.title(\"Bayes Optimization Validation Scores\");\n",
    "plt.xlim(0.75, 0.78);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does appear that the validation ROC AUC for the Bayesian optimization is higher than that for Random Search. However, as we have seen, this does not necessarily translate to a better testing score! \n",
    "\n",
    "Bayesian optimization should get better over time. Let's plot the scores against the iteration to see if there was improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of scores over the course of searching\n",
    "sns.lmplot('iteration', 'ROC AUC', hue = 'search', data = scores, size = 8);\n",
    "plt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"ROC AUC versus Iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's reassuring to see that the validation ROC AUC scores of Bayesian optimization increase over time. What this shows is that the model is exploring hyperparameters that are better according to the cross validation metric! It would be interesting to continue searching and see if there is a plateau in the validation scores (there would have to be eventually). Moreover, even if validation scores continue to increase, that does not mean a better model for the testing data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to save to save the trials results, we can use the json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the trial results\n",
    "with open('results/trials.json', 'w') as f:\n",
    "    f.write(json.dumps(bayes_trials.results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes of parameters\n",
    "bayes_params.to_csv('results/bayes_params.csv', index = False)\n",
    "random_params.to_csv('results/random_params.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Searching\n",
    "\n",
    "We can keep running the Bayesian hyperparameter search for more iterations to try for better results. Hyperopt will continue searching where it left off if we [pass it a trials object that already has information on previous runs](https://github.com/hyperopt/hyperopt/issues/267). This raises a good point: always save your previous results, because you never know when they will be useful! \n",
    "\n",
    "Another interesting point to not is that Bayesian Optimization methods do not have any internal state which means all they need are the results: previous inputs to the objective function and the resulting loss. Based only on these results, these methods can construct a surrogate function and suggest the next set of hyperparameters to evaluate. The internals of the objective function have no effect on the Bayesian Optimization method hence the naming of this as a black box optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "ITERATION = MAX_EVALS + 1\n",
    "\n",
    "# Set more evaluations\n",
    "MAX_EVALS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Use the same trials object to keep training\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, verbose = 1, rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results/gbm_trials.csv')\n",
    "\n",
    "# Sort values with best on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "best_bayes_model.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from Bayes optimization scores 0.72736 AUC ROC on the test set.\n",
      "This was achieved after 846 search iterations\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the testing data \n",
    "preds = best_bayes_model.predict_proba(test_features)[:, 1]\n",
    "print('The best model from Bayes optimization scores {:.5f} AUC ROC on the test set.'.format(roc_auc_score(test_labels, preds)))\n",
    "print('This was achieved after {} search iterations'.format(results.loc[0, 'iteration']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuation of the search did slightly improve the validation score (again depending on training run). Instead of training more, we might want to restart the search so the algorithm can spend more time exploring the domain space. As searching continues, the algorithm shifts from exploring (trying new values) to exploiting (trying those values that worked best in the past). This is generally what we want unless the model gets stuck in a local minimum at which point we would want to restart the search in a different region of the hyperparameter space. Bayesian Optimization of hyperparameters is still prone to overfitting, even when using cross-validation because it can get settle into a local minimum of the objective function. It is very difficult to tell when this occurs for a high-dimensional problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to implement automated hyperparameter tuning with Bayesian Optimization methods. We used the open-source Python library Hyperopt with the Tree Parzen Estimator to optimize the hyperparameters of a gradient boosting machine. \n",
    "\n",
    "Bayesian model-based optimization can be more efficient than random search, finding a better set of model hyperparameters in fewer search iterations (although not in every case). However, just because the model hyperparameters are better on the validation set does not mean they are better for the testing set! For this training run, Bayesian Optimization found a better set of hyperparamters according to the validation and the test data although the testing score was much lower than the validation ROC AUC. This is a useful lesson that even when using cross-validation, overfitting is still one of the top problems in machine learning. \n",
    "\n",
    "Bayesian optimization  is a powerful technique that we can use to tune any machine learning model, so long as we can define an objective function that returns a value to minimize and a domain space over which to search. This can extend to any function that we want to minimize (not just hyperparameter tuning). Bayesian optimization can be a significant upgrade over uninformed methods such as random search and because of the ease of use in Python are now a good option to use for hyperparameter tuning. As with most subjects in machine learning, there is no single best answer for hyperparameter tuning, but Bayesian optimization methods should be a tool that helps data scientists with the tedious but necessary task of model tuning! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
